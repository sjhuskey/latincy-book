[
  {
    "objectID": "2_install.html",
    "href": "2_install.html",
    "title": "1  Installing LatinCy models",
    "section": "",
    "text": "1.1 Installing spaCy\nThe LatinCy models are designed to work with the spaCy natural language platform, so you will need to have this package installed before anything else. The following cell has the pip install command for spaCy. At the time of writing, the latest version available for spaCy compatible with LatinCy is v3.7.5.\nNB: To run the cells below, uncomment the commands by removing the # at the beginning of the line. The exclamation point at the beginning of the line is shorthand for the %system magic command in Jupyter which can be used to run shell commands from within a notebook.\n# !pip install -U spacy",
    "crumbs": [
      "Model/Pipeline Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing LatinCy models</span>"
    ]
  },
  {
    "objectID": "2_install.html#installing-the-latincy-models",
    "href": "2_install.html#installing-the-latincy-models",
    "title": "1  Installing LatinCy models",
    "section": "1.2 Installing the LatinCy models",
    "text": "1.2 Installing the LatinCy models\nLatinCy models are currently available in three sizes: ‘sm’, ‘md’, and ‘lg’. We will use the different models throughout the tutorials, so let’s install all three now so that they are available for future chapters.\n\n# ! pip install \"la-core-web-sm @ https://huggingface.co/latincy/la_core_web_sm/resolve/main/la_core_web_sm-any-py3-none-any.whl\"\n# ! pip install \"la-core-web-md @ https://huggingface.co/latincy/la_core_web_md/resolve/main/la_core_web_md-any-py3-none-any.whl\"\n# ! pip install \"la-core-web-lg @ https://huggingface.co/latincy/la_core_web_lg/resolve/main/la_core_web_lg-any-py3-none-any.whl\"\n# ! pip install \"la-core-web-trf @ https://huggingface.co/latincy/la_core_web_trf/resolve/main/la_core_web_trf-any-py3-none-any.whl\"",
    "crumbs": [
      "Model/Pipeline Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing LatinCy models</span>"
    ]
  },
  {
    "objectID": "2_install.html#installing-additional-packages",
    "href": "2_install.html#installing-additional-packages",
    "title": "1  Installing LatinCy models",
    "section": "1.3 Installing additional packages",
    "text": "1.3 Installing additional packages\nWe will also use other packages throughout these tutorials. They are included here for your convenience, as well as in the requirements.txt file in the code repository for this Quarto book.\n\n# !pip install pandas\n# !pip install matplotlib\n# !pip install seaborn\n# !pip install scikit-learn\n# !pip install tqdm",
    "crumbs": [
      "Model/Pipeline Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Installing LatinCy models</span>"
    ]
  },
  {
    "objectID": "3_load.html",
    "href": "3_load.html",
    "title": "2  Loading LatinCy models",
    "section": "",
    "text": "2.1 Loading LatinCy models with spaCy\n# Imports\n\nimport spacy\nfrom pprint import pprint\nnlp = spacy.load('la_core_web_lg')",
    "crumbs": [
      "Model/Pipeline Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Loading LatinCy models</span>"
    ]
  },
  {
    "objectID": "3_load.html#creating-a-spacy-doc-from-a-string",
    "href": "3_load.html#creating-a-spacy-doc-from-a-string",
    "title": "2  Loading LatinCy models",
    "section": "2.2 Creating a spaCy doc from a string",
    "text": "2.2 Creating a spaCy doc from a string\n\ntext = \"Haec narrantur a poetis de Perseo.\"\ndoc = nlp(text)\nprint(doc)\n\nHaec narrantur a poetis de Perseo.\n\n\n\nprint(type(doc))\n\n&lt;class 'spacy.tokens.doc.Doc'&gt;\n\n\n\nprint(doc.__repr__())\n\nHaec narrantur a poetis de Perseo.\n\n\n\nprint(doc.text)\n\nHaec narrantur a poetis de Perseo.\n\n\n\nprint(type(doc.text))\n\n&lt;class 'str'&gt;",
    "crumbs": [
      "Model/Pipeline Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Loading LatinCy models</span>"
    ]
  },
  {
    "objectID": "sentence-segmentation.html",
    "href": "sentence-segmentation.html",
    "title": "3  Sentence segmentation",
    "section": "",
    "text": "3.1 Sentence segmentation with LatinCy\nSentence segmentation is the task of splitting a text into sentences. For the LatinCy models, this is a task been trained using spaCy’s senter factory to terminate sentences at both strong and weak stops, following the example of Clayman (1981) (see also, Wake (1957), Janson (1964)), who writes: “If all stops are made equivalent, i.e. if no distinction is made between the strong stop, weak stop and interrogation mark, editorial differences will be kept to a minimum.”\nGiven a spaCy Doc, the sents attribute will produce a generator object with the sentence from that document as determined by the dependency parser. Each sentence is a Span object with the start and end token indices from the original Doc.\n# Imports & setup\n\nimport spacy\nfrom pprint import pprint\nnlp = spacy.load('la_core_web_lg')\ntext = \"Haec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum. Avus eius Acrisius appellabatur.\"\ndoc = nlp(text)\nprint(doc)\n\nHaec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum. Avus eius Acrisius appellabatur.\nsents = doc.sents\nprint(type(sents))\n\n&lt;class 'generator'&gt;\nLike all Span objects, the text from each sentence can be retrieved with the text attribute. For convenience below, we convert the generator to list so that we can iterate over it multiple times. Here are the three (3) sentences identified in the example text as well as an indication of the sentences’ type, i.e. &lt;class 'spacy.tokens.span.Span'&gt;.\nsents = list(sents)\n\nfor i, sent in enumerate(sents, 1):\n    print(f'{i}: {sent.text}')\n\n1: Haec narrantur a poetis de Perseo.\n2: Perseus filius erat Iovis, maximi deorum.\n3: Avus eius Acrisius appellabatur.\nsent = sents[0]\nprint(type(sent))\n\n&lt;class 'spacy.tokens.span.Span'&gt;\nSentences have the same atrributes/methods available to them as any span (listed in the next cell). Following are some attibutes/methods that may be particularly relevant to working with sentences.\nsent_methods = [item for item in dir(sent) if '_' not in item]\npprint(sent_methods)\n\n['conjuncts',\n 'doc',\n 'end',\n 'ents',\n 'id',\n 'label',\n 'lefts',\n 'rights',\n 'root',\n 'sent',\n 'sentiment',\n 'sents',\n 'similarity',\n 'start',\n 'subtree',\n 'tensor',\n 'text',\n 'vector',\n 'vocab']\nYou can identify the root of the sentence as determined by the dependency parser. Assuming the parsing in correct, this will be the main verb of the sentence.\nprint(sent.root)\n\nnarrantur\nEach word in the sentence has an associated vector. Sentence (any Span in fact) has an associated vector as well that is the mean of the vectors of the words in the sentence. As this example uses the lg model, the vector has a length of 300.\nprint(sent.vector.shape)\n\n(300,)\nThis vector then can be used to compute the similarity between two sentences. Here we see our example sentence compared to two related sentence: 1. a sentence where the character referred to is changed from Perseus to Ulysses; and 2. the active-verb version of the sentence.\nsent.similarity(nlp('Haec narrantur a poetis de Ulixe.'))\n\n0.9814933448498585\nsent.similarity(nlp('Haec narrant poetae de Perseo.'))\n\n0.7961655550941479\nWe can retrieve the start and end indices from the original document for each sentence.\nsent_2 = sents[1]\nstart = sent_2.start\nend = sent_2.end\nprint(start)\nprint(end)\nprint(sent_2.text)\nprint(doc[start:end].text)\n\n7\n15\nPerseus filius erat Iovis, maximi deorum.\nPerseus filius erat Iovis, maximi deorum.",
    "crumbs": [
      "NLP Tasks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sentence segmentation</span>"
    ]
  },
  {
    "objectID": "sentence-segmentation.html#sentence-segmentation-with-latincy",
    "href": "sentence-segmentation.html#sentence-segmentation-with-latincy",
    "title": "3  Sentence segmentation",
    "section": "",
    "text": "References\nSLP Chapter 2, Section 2.4.5 “Sentence Segmentation”, pp. 24 [link] (https://web.stanford.edu/~jurafsky/slp3/)\nspaCy SentenceRecognizer\n\n\n\n\nClayman, Dee. 1981. “Sentence Length in Greek Hexameter Poetry.” Quantitative Linguistics 11: 107–36. https://papers.ssrn.com/abstract=1627358.\n\n\nJanson, Tore. 1964. “The Problems of Measuring Sentence-Length in Classical Texts.” Studia Linguistica 18 (1): 26–36. https://doi.org/10.1111/j.1467-9582.1964.tb00443.x.\n\n\nWake, William C. 1957. “Sentence-Length Distributions of Greek Authors.” Journal of the Royal Statistical Society. Series A (General) 120 (3): 331–46. https://www.jstor.org/stable/2343104.",
    "crumbs": [
      "NLP Tasks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sentence segmentation</span>"
    ]
  },
  {
    "objectID": "word-tokenization.html",
    "href": "word-tokenization.html",
    "title": "4  Word Tokenization",
    "section": "",
    "text": "4.1 Word tokenization with LatinCy\n# Imports & setup\n\nimport spacy\nfrom pprint import pprint\nnlp = spacy.load('la_core_web_md')\ntext = \"Haec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum. Avus eius Acrisius appellabatur.\"\ndoc = nlp(text)\nprint(doc)\n\nHaec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum. Avus eius Acrisius appellabatur.\nWord tokenization is the task of splitting a text into words (and wordlike units like punctuation, numbers, etc.). For the LatinCy models, tokenization is the fundamental pipeline component on which all other components depend. SpaCy uses non-destructive, “canonical” tokenization, i.e. non-destructive, in that the original text can be untokenized, so to speak, based on Token annotations and canonical in that indices are assigned to each token during this process and these indices are used to refer to the tokens in other annotations. (Tokens can be separated or merged, but this requires the user to actively undo and redefine the tokenization output.) LatinCy uses a modified version of the default spaCy tokenizer that recognizes and splits enlitic -que using a rules-based process. (NB: It is in the LatinCy development plan to move enclitic splitting to a separate post-tokenization component.)\nThe spaCy Doc object is an iterable and tokens are the iteration unit.\ntokens = [item for item in doc]\nprint(tokens)\n\n[Haec, narrantur, a, poetis, de, Perseo, ., Perseus, filius, erat, Iovis, ,, maximi, deorum, ., Avus, eius, Acrisius, appellabatur, .]\ntoken = tokens[0]\nprint(type(token))\n\n&lt;class 'spacy.tokens.token.Token'&gt;\nThe text content of a Token object can be retrieved with the text attribute.\nfor i, token in enumerate(tokens, 1):\n    print(f'{i}: {token.text}')\n\n1: Haec\n2: narrantur\n3: a\n4: poetis\n5: de\n6: Perseo\n7: .\n8: Perseus\n9: filius\n10: erat\n11: Iovis\n12: ,\n13: maximi\n14: deorum\n15: .\n16: Avus\n17: eius\n18: Acrisius\n19: appellabatur\n20: .\nNote again that the token itself is a spaCy Token object and that the text attribute returns a Python string even though their representations in the Jupyter Notebook look the same.\ntoken = tokens[0]\nprint(f'{type(token)} -&gt; {token}')\nprint(f'{type(token.text)} -&gt; {token.text}')\n\n&lt;class 'spacy.tokens.token.Token'&gt; -&gt; Haec\n&lt;class 'str'&gt; -&gt; Haec",
    "crumbs": [
      "NLP Tasks",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Word Tokenization</span>"
    ]
  },
  {
    "objectID": "word-tokenization.html#word-tokenization-with-latincy",
    "href": "word-tokenization.html#word-tokenization-with-latincy",
    "title": "4  Word Tokenization",
    "section": "",
    "text": "4.1.1 Token attributes and methods related to tokenization\nHere are some atrributes/methods available for spaCy Token objects that are relevant to word tokenization.\nSpaCy keeps track of both the token indices and the character offsets within a doc using either the i or idx attributes, respectively…\n\nprint(token.doc)\n\nHaec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum. Avus eius Acrisius appellabatur.\n\n\n\n# token indices\n\nfor token in doc:\n    print(f'{token.i}: {token.text}')\n\n0: Haec\n1: narrantur\n2: a\n3: poetis\n4: de\n5: Perseo\n6: .\n7: Perseus\n8: filius\n9: erat\n10: Iovis\n11: ,\n12: maximi\n13: deorum\n14: .\n15: Avus\n16: eius\n17: Acrisius\n18: appellabatur\n19: .\n\n\nThis is functionally equivalent to using enumerate…\n\n# token indices, with enumerate\n\nfor i, token in enumerate(doc):\n    print(f'{i}: {token.text}')\n\n0: Haec\n1: narrantur\n2: a\n3: poetis\n4: de\n5: Perseo\n6: .\n7: Perseus\n8: filius\n9: erat\n10: Iovis\n11: ,\n12: maximi\n13: deorum\n14: .\n15: Avus\n16: eius\n17: Acrisius\n18: appellabatur\n19: .\n\n\nAnother indexing option is the idx attribute which is the character offset of the token in the original Doc object.\n\n# character offsets, \nfor token in doc:\n    print(f'{token.idx}: {token.text}')\n\n0: Haec\n5: narrantur\n15: a\n17: poetis\n24: de\n27: Perseo\n33: .\n35: Perseus\n43: filius\n50: erat\n55: Iovis\n60: ,\n62: maximi\n69: deorum\n75: .\n77: Avus\n82: eius\n87: Acrisius\n96: appellabatur\n108: .\n\n\nObserve these idx attributes relate to the character offsets from the original Doc. To illustrate the point, we will replace spaces with an underscore in the output. We can see from the output above that narrantur begins at idx 5 and that the next word a begins at idx 15. Yet narrantur is only 9 characters long and the difference between these two numbers is 10! This is because we need to account for whitespace in the original Doc. This is handled by the attribute text_with_ws.\n\nprint(doc.text[5:15].replace(' ', '_'))\n\nnarrantur_\n\n\n\n\nprint(f'text -&gt; {doc[1].text} (length {len(doc[1].text)})')\nprint()\nprint(f'text_with_ws -&gt; {doc[1].text_with_ws} (length {len(doc[1].text_with_ws)})')\n\ntext -&gt; narrantur (length 9)\n\ntext_with_ws -&gt; narrantur  (length 10)\n\n\nAccordingly, using the text_with_ws attribute (as opposed to simply the text attribute) we can reconstruct the original text. This is what was meant above by “non-destructive” tokenization. Look at the difference between a text joined using the text attribute and one joined using the text_with_ws attribute.\n\njoined_tokens = ' '.join([token.text for token in doc])\nprint(joined_tokens)\nprint(joined_tokens == doc.text)\n\nprint()\n\nreconstructed_text = ''.join([token.text_with_ws for token in doc])\nprint(reconstructed_text)\nprint(reconstructed_text == doc.text)\n\nHaec narrantur a poetis de Perseo . Perseus filius erat Iovis , maximi deorum . Avus eius Acrisius appellabatur .\nFalse\n\nHaec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum. Avus eius Acrisius appellabatur.\nTrue\n\n\nBecause spaCy tokenization is set from the outset, you can traverse the tokens in a Doc objects from the tokens themselves using the nbor method. This method takes an integer argument that specifies the number of tokens to traverse. A positive integer traverses the tokens to the right, a negative integer traverses the tokens to the left.\n\nprint(doc[:6])\nprint('-----')\nprint(f'{doc[3]}, i.e. i = 3')\nprint(f'{doc[3].nbor(-1)}, i.e. i - 1 = 2')\nprint(f'{doc[3].nbor(-2)}, i.e. i - 2 = 1')\nprint(f'{doc[3].nbor(1)}, i.e. i + 1 = 4')\nprint(f'{doc[3].nbor(2)}, i.e. i + 2 = 5')\n\nHaec narrantur a poetis de Perseo\n-----\npoetis, i.e. i = 3\na, i.e. i - 1 = 2\nnarrantur, i.e. i - 2 = 1\nde, i.e. i + 1 = 4\nPerseo, i.e. i + 2 = 5\n\n\n\n\n4.1.2 Customization of the spaCy tokenizer in LatinCy\nLatinCy aims to tokenize the que enclitic in Latin texts. As noted above this is currently done through a rule-based approach. Here is the custom tokenizer code (beginning at this line in the code) followed by a description of the process. Note that this process is based on the following recommendations in the spaCy documentation: https://spacy.io/usage/training#custom-tokenizer.\n\nfrom spacy.util import registry, compile_suffix_regex\n\n@registry.callbacks(\"customize_tokenizer\")\ndef make_customize_tokenizer():\n    def customize_tokenizer(nlp):\n        suffixes = nlp.Defaults.suffixes + [\n            \"que\",\n            \"qve\",\n        ]\n        suffix_regex = compile_suffix_regex(suffixes)\n        nlp.tokenizer.suffix_search = suffix_regex.search\n\n        for item in que_exceptions:\n            nlp.tokenizer.add_special_case(item, [{\"ORTH\": item}])\n            nlp.tokenizer.add_special_case(item.lower(), [{\"ORTH\": item.lower()}])\n            nlp.tokenizer.add_special_case(item.title(), [{\"ORTH\": item.title()}])\n            nlp.tokenizer.add_special_case(item.upper(), [{\"ORTH\": item.upper()}])\n\n    return customize_tokenizer\n\nBasically, we treat que (and its case and u/v norm variants) as punctuation. These are added to the Defaults.suffixes. If no other intervention were made, then any word ending in que or a variant would be split into a before-que part and que. Since there are large number of relatively predictable words that end in que, these are maintained in a list called que_exceptions. All of the words in the que_exceptions list are added as a “special case” using the tokenizer’s add_special_case method and so will not be split. The que_exceptions lists is as follows:\n\nque_exceptions = ['quisque', 'quidque', 'quicque', 'quodque', 'cuiusque', 'cuique', 'quemque', 'quamque', 'quoque', 'quaque', 'quique', 'quaeque', 'quorumque', 'quarumque', 'quibusque', 'quosque', 'quasque', 'uterque', 'utraque', 'utrumque', 'utriusque', 'utrique', 'utrumque', 'utramque', 'utroque', 'utraque', 'utrique', 'utraeque', 'utrorumque', 'utrarumque', 'utrisque', 'utrosque', 'utrasque', 'quicumque', 'quidcumque', 'quodcumque', 'cuiuscumque', 'cuicumque', 'quemcumque', 'quamcumque', 'quocumque', 'quacumque', 'quicumque', 'quaecumque', 'quorumcumque', 'quarumcumque', 'quibuscumque', 'quoscumque', 'quascumque', 'unusquisque', 'unaquaeque', 'unumquodque', 'unumquidque', 'uniuscuiusque', 'unicuique', 'unumquemque', 'unamquamque', 'unoquoque', 'unaquaque', 'plerusque', 'pleraque', 'plerumque', 'plerique', 'pleraeque', 'pleroque', 'pleramque', 'plerorumque', 'plerarumque', 'plerisque', 'plerosque', 'plerasque', 'absque', 'abusque', 'adaeque', 'adusque', 'aeque', 'antique', 'atque', 'circumundique', 'conseque', 'cumque', 'cunque', 'denique', 'deque', 'donique', 'hucusque', 'inique', 'inseque', 'itaque', 'longinque', 'namque', 'neque', 'oblique', 'peraeque', 'praecoque', 'propinque', 'qualiscumque', 'quandocumque', 'quandoque', 'quantuluscumque', 'quantumcumque', 'quantuscumque', 'quinque', 'quocumque', 'quomodocumque', 'quomque', 'quotacumque', 'quotcumque', 'quotienscumque', 'quotiensque', 'quotusquisque', 'quousque', 'relinque', 'simulatque', 'torque', 'ubicumque', 'ubique', 'undecumque', 'undique', 'usque', 'usquequaque', 'utcumque', 'utercumque', 'utique', 'utrimque', 'utrique', 'utriusque', 'utrobique', 'utrubique']\n\nYou can see these words in the rules attribute of the tokenizer.\n\n# Sample of 10 que rules from the custom tokenizer\n\ntokenizer_rules = nlp.tokenizer.rules\nprint(sorted(list(set([rule.lower() for rule in tokenizer_rules if 'que' in rule])))[:10])\n\n['absque', 'abusque', 'adaeque', 'adusque', 'aeque', 'antique', 'atque', 'circumundique', 'conseque', 'cuicumque']\n\n\nWith the exception of basic enclitic splitting, the LatinCy tokenizer is the same as the default spaCy tokenizer. The default spaCy tokenizer is described in detail in the spaCy documentation. Here are some useful attributes/methods for working with LatinCy.\nTokenize a string without any other pipeline annotations with a call.\n\ntokens = nlp.tokenizer(text)\nprint(tokens)\nprint(tokens[0].text)\nprint(tokens[0].lemma_) # Note that there is no annotation here because since the tokenizer has been called directly, the lemmatizer—the entire pipeline, in fact—has not been run\n\nHaec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum. Avus eius Acrisius appellabatur.\nHaec\n\n\n\nA list of texts can be tokenized in one pass with the pipe method. This yields a generator object where each item is Doc object of tokenized-only texts\n\ntexts = [\"Haec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum; avus eius Acrisius appellabatur. Acrisius volebat Perseum nepotem suum necare; nam propter oraculum puerum timebat. Comprehendit igitur Perseum adhuc infantem, et cum matre in arca lignea inclusit. Tum arcam ipsam in mare coniecit. Danae, Persei mater, magnopere territa est; tempestas enim magna mare turbabat. Perseus autem in sinu matris dormiebat.\", \"Iuppiter tamen haec omnia vidit, et filium suum servare constituit. Tranquillum igitur fecit mare, et arcam ad insulam Seriphum perduxit. Huius insulae Polydectes tum rex erat. Postquam arca ad litus appulsa est, Danae in harena quietem capiebat. Post breve tempus a piscatore quodam reperta est, et ad domum regis Polydectis adducta est. Ille matrem et puerum benigne excepit, et iis sedem tutam in finibus suis dedit. Danae hoc donum libenter accepit, et pro tanto beneficio regi gratias egit.\", \"Perseus igitur multos annos ibi habitabat, et cum matre sua vitam beatam agebat. At Polydectes Danaen magnopere amabat, atque eam in matrimonium ducere volebat. Hoc tamen consilium Perseo minime gratum erat. Polydectes igitur Perseum dimittere constituit. Tum iuvenem ad se vocavit et haec dixit: \\\"Turpe est hanc ignavam vitam agere; iam dudum tu adulescens es. Quo usque hic manebis? Tempus est arma capere et virtutem praestare. Hinc abi, et caput Medusae mihi refer.\\\"\"]\n\ntokens = list(nlp.tokenizer.pipe(texts))\n\nprint(len(tokens)) # number of documents\nprint(len(tokens[0])) # number of tokens in first document\n\n3\n76\n\n\nYou can get an explanation of the tokenization “decisions” using the explain method. In the example below, we see how the que in virumque is treated as a suffix (as discussed above) and so is split during tokenization.\n\ntok_exp = nlp.tokenizer.explain('arma virumque cano')\nprint(tok_exp)\n\n[('TOKEN', 'arma'), ('TOKEN', 'virum'), ('SUFFIX', 'que'), ('TOKEN', 'cano')]\n\n\n\ntokens = nlp.tokenizer('arma uirumque cano')\nfor i, token in enumerate(tokens):\n    print(f'{i}: {token.text}')\n\n0: arma\n1: uirum\n2: que\n3: cano\n\n\n\n\nReferences\nSLP Chapter 2, Section 2.4.2 “Word Tokenization”, pp. 18-20 link",
    "crumbs": [
      "NLP Tasks",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Word Tokenization</span>"
    ]
  },
  {
    "objectID": "lemmatization.html",
    "href": "lemmatization.html",
    "title": "5  Lemmatization",
    "section": "",
    "text": "5.1 Lemmatization with LatinCy\nLemmatization is the task of mapping a token in a text to its dictionary headword. With the default LatinCy pipelines, two components are used to perform this task: 1. spaCy’s Edit Tree Lemmatizer and 2. a second custom Lookup Lemmatizer, named in the pipeline “trainable_lemmatizer” and “lookup_lemmatizer” respectively. In the first lemmatization pass, a probabilistic tree model is used to predict the transformation from the token form to its lemma. A second pass is made at the end of the pipeline which checks the token form against a large (~1M item) lemma dictionary (i.e. lookups) for ostensibly unambiguous forms; if a match is found, the lemma is overwritten with the corresponding value from lookup. The two-pass logic largely follows the approach recommended in Burns (2018) and Burns (2020), as facilitated by the spaCy pipeline architecture.\n# Imports & setup\n\nimport spacy\nfrom pprint import pprint\nnlp = spacy.load('la_core_web_sm')\ntext = \"Haec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum. Avus eius Acrisius appellabatur.\"\ndoc = nlp(text)\nprint(doc)\n\nHaec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum. Avus eius Acrisius appellabatur.\nNote here the two lemmatizer components that are included in the pipeline, i.e. “trainable_lemmatizer” and “lookup_lemmatizer”…\nprint(nlp.pipe_names)\n\n['senter', 'normer', 'tok2vec', 'tagger', 'morphologizer', 'trainable_lemmatizer', 'parser', 'lookup_lemmatizer', 'ner']\nOnce a text is annotated using the LatinCy pipeline, i.e. as part of the Doc creation process, lemmas can be found as annotations of the Token objects…\nsample_token = doc[0]\n\nprint(f'Sample token: {sample_token.text}')\nprint(f'Sample lemma: {sample_token.lemma_}')\n\nSample token: Haec\nSample lemma: hic\nimport tabulate\n\ndata = []\n\ntokens = [item for item in doc]\n\nfor token in tokens:\n    data.append([token.text, token.lemma_])    \n\nprint(tabulate.tabulate(data, headers=['Text', 'Lemma']))\n\nText          Lemma\n------------  --------\nHaec          hic\nnarrantur     narro\na             ab\npoetis        poeta\nde            de\nPerseo        Perseo\n.             .\nPerseus       Perseus\nfilius        filius\nerat          sum\nIovis         Iuppiter\n,             ,\nmaximi\ndeorum        deus\n.             .\nAvus          auus\neius          is\nAcrisius      Acrisius\nappellabatur  appello\n.             .\nThe lemma_ attribute has the type str and so is compatible with all string operations…\nprint(f'Token: {tokens[0].text}')\nprint(f'Lemma: {tokens[0].lemma_}')\nprint(f'Lowercase lemma: {tokens[0].lemma_.lower()}')\n\nToken: Haec\nLemma: hic\nLowercase lemma: hic\nThe lemma_ attribute, though, is only the human-readable version of the lemma. Internally, spaCy uses a hash value to represent the lemma, which is stored in the lemma attribute. (Note the lack of trailing underscore.)\nprint(f'Token: {tokens[1].text}')\nprint(f'Human-readable lemma: {tokens[1].lemma_}')\nprint(f'spaCy lemma key: {tokens[1].lemma}')\n\nToken: narrantur\nHuman-readable lemma: narro\nspaCy lemma key: 11361982710182407617\nIn order to compare the two different lemmatization passes, we can create two copies of the LatinCy pipeline, each with one of the two lemmatizers removed…\nimport copy\n\nP1 = copy.deepcopy(nlp)\nP1.disable_pipes((['tagger', 'morphologizer', 'lookup_lemmatizer']))\nprint(f'First pipeline components: {P1.pipe_names}')\n\nP2 = copy.deepcopy(nlp)\nP2.disable_pipes((['tagger', 'morphologizer', 'trainable_lemmatizer']))\nprint(f'Second pipeline components: {P2.pipe_names}')\n\nFirst pipeline components: ['senter', 'normer', 'tok2vec', 'trainable_lemmatizer', 'parser', 'ner']\nSecond pipeline components: ['senter', 'normer', 'tok2vec', 'parser', 'lookup_lemmatizer', 'ner']\nWe can then run the same text through both pipelines and compare the results side-by-side…\nP1_annotations = P1(text)\nP2_annotations = P2(text)\n\ndata = []\n\nfor p1_token, p2_token in zip(P1_annotations, P2_annotations):\n    data.append([p1_token.text, p1_token.lemma_, p2_token.lemma_])\n\nprint(tabulate.tabulate(data, headers=['Text', 'Trainable lemmatizer', 'Lookup lemmatizer']))\n\nText          Trainable lemmatizer    Lookup lemmatizer\n------------  ----------------------  -------------------\nHaec          hic\nnarrantur     narro                   narro\na             ab\npoetis        poetus                  poeta\nde            de\nPerseo        Perseo\n.             .                       .\nPerseus       Perseus\nfilius        filius                  filius\nerat          sum                     sum\nIovis         Iovis                   Iuppiter\n,             ,                       ,\nmaximi\ndeorum        deus                    deus\n.             .                       .\nAvus          avus                    auus\neius          is                      is\nAcrisius      Acrisius\nappellabatur  appello                 appello\n.             .                       .\nNote specifically the lemmatization of Iovis—since it is a highly irregular form, it is not surprising that the Edit Tree Lemmatizer has manufactured a potential, but incorrect, lemma based on the root Iov-. Since Iovis is an unambiguous Latin form and has been added to the LatinCy lookups, the Lookup Lemmatizer steps in to correct the erroneous first pass. The lookup data can be found a custom form of the spaCy spacy-lookups-data package [https://github.com/diyclassics/spacy-lookups-data/tree/master/spacy_lookups_data/data]. These lookups are installed as a dependency of each of the LatinCy pipelines.\nThe code below shows how to access the lookup data directly…\n# Load the lookups data\n\nfrom spacy.lookups import Lookups, load_lookups\n\nblank_nlp = spacy.blank(\"la\")\nlookups = Lookups()\n\nlookups_data = load_lookups(lang=blank_nlp.vocab.lang, tables=[\"lemma_lookup\"])\nLOOKUPS = lookups_data.get_table(\"lemma_lookup\")\n\nprint(LOOKUPS['Iovis'])\n\nIuppiter",
    "crumbs": [
      "NLP Tasks",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Lemmatization</span>"
    ]
  },
  {
    "objectID": "lemmatization.html#lemmatization-with-latincy",
    "href": "lemmatization.html#lemmatization-with-latincy",
    "title": "5  Lemmatization",
    "section": "",
    "text": "References\nNLTK Chapter 3, Section 3.6 “Normalizing text” link\nSLP Chapter 2, Section 2.6 “Word normalization, lemmatization and stemming”, pp. 23-24 link\nspaCy EditTreeLemmatizer link\n\n\n\n\nBurns, Patrick J. 2018. “Backoff Lemmatization as a Philological Method.” In Digital Humanities 2018, DH 2018, Book of Abstracts, El Colegio de México, UNAM, and RedHD, Mexico City, Mexico, June 26-29, 2018.\n\n\n———. 2020. “Ensemble Lemmatization with the Classical Language Toolkit.” Studi e Saggi Linguistici 58 (1): 157–76. https://doi.org/10.4454/ssl.v58i1.273.",
    "crumbs": [
      "NLP Tasks",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Lemmatization</span>"
    ]
  },
  {
    "objectID": "pos-tagging.html",
    "href": "pos-tagging.html",
    "title": "6  POS Tagging",
    "section": "",
    "text": "6.1 POS Tagging with LatinCy\nPart-of-speech tagging is the task of mapping a token in a text to its part of speech, whether ‘noun’ or ‘verb’ or ‘preposition’ and so on. There are two components in the default LatinCy pipelines that provide such annotations, that is the tagger and the morphologizer. Ostensibly, the tagger provides language-specific, fine-grain POS tags and the morphologizer provides coarse-grain tags (as defined by the UD Universal POS tags); at present, the LatinCy models have a high degree of overlap between these two tagsets and there are effectively no fine-grain tags.\n# Imports & setup\n\nimport spacy\nfrom pprint import pprint\nnlp = spacy.load('la_core_web_lg')\ntext = \"Haec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum. Avus eius Acrisius appellabatur.\"\ndoc = nlp(text)\nprint(doc)\n\nHaec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum. Avus eius Acrisius appellabatur.\nNote here the two “tagging” components that are included in the pipeline, i.e. “tagger” and “morphologizer”…\nprint(nlp.pipe_names)\n\n['senter', 'normer', 'tok2vec', 'tagger', 'morphologizer', 'trainable_lemmatizer', 'parser', 'lookup_lemmatizer', 'ner']\nOnce a text is annotated using the LatinCy pipeline, i.e. as part of the Doc creation process, tags can be found as annotations of the Token objects. The coarse-grain tags are stored in the pos_ attribute; fine-grain tags are stored in the tag_ attribute.\nsample_token = doc[1]\n\nprint(f'Sample token: {sample_token.text}')\nprint(f'Sample POS: {sample_token.pos_}')\nprint(f'Sample TAG: {sample_token.tag_}')\n\nSample token: narrantur\nSample POS: VERB\nSample TAG: verb\nNote the high degree of overlap between the coarse-grain and fine-grain tags in the LatinCy models in the chart below. That said, it is perhaps worth paying more attention to where the tagsets do not overlap. In the LatinCy training, conventions for classes of words to be labeled, say, “DET” (as in the Haec pos_) or “AUX” (as in est) are inferred from usage in the six different treebanks used. A sense of the inconsistency in the tagsets can be gleaned from the following page: (https://universaldependencies.org/la/); note also the important work of Gamba and Zeman (2023) in this area.\nimport tabulate\n\ndata = []\n\ntokens = [item for item in doc]\n\nfor token in tokens:\n    data.append([token.text, token.pos_, token.tag_])    \n\nprint(tabulate.tabulate(data, headers=['Text', \"POS\", \"TAG\"]))  \n\nText          POS    TAG\n------------  -----  -----------\nHaec          DET    pronoun\nnarrantur     VERB   verb\na             ADP    preposition\npoetis        NOUN   noun\nde            ADP    preposition\nPerseo        PROPN  proper_noun\n.             PUNCT  punc\nPerseus       PROPN  proper_noun\nfilius        NOUN   noun\nerat          AUX    verb\nIovis         PROPN  proper_noun\n,             PUNCT  punc\nmaximi        ADJ    adjective\ndeorum        NOUN   noun\n.             PUNCT  punc\nAvus          NOUN   noun\neius          PRON   pronoun\nAcrisius      PROPN  proper_noun\nappellabatur  VERB   verb\n.             PUNCT  punc\nAs with the lemma annotations, the pos_ and tag_ attributes are only the human-readable of the lemma. Internally, spaCy uses a hash value to represent this, again noting the lack of trailing underscore…\nprint(f\"Token: {tokens[1].text}\")\nprint(f'Human-readable TAG: {tokens[1].tag_}')\nprint(f'spaCy TAG key: {tokens[1].tag}')\n\nToken: narrantur\nHuman-readable TAG: verb\nspaCy TAG key: 6360137228241296794\nThis tag ‘key’ can be looked up in spaCy’s NLP.vocab.strings atrribute…\nT = nlp.get_pipe('tagger')\ntag_lookup = T.vocab.strings[6360137228241296794] # also would work on `nlp`, i.e. nlp.vocab.strings[6360137228241296794]\n\nprint(f'TAG key: {tokens[1].tag}')\nprint(f'Human-readable TAG: {tag_lookup}')\n\nTAG key: 6360137228241296794\nHuman-readable TAG: verb\nThe same process applies to the POS ‘keys’…\nprint(f'Token: {tokens[1].text}')\nprint(f'Human-readable POS: {tokens[1].pos_}')\nprint(f'spaCy POS key: {tokens[1].pos}')\n\nToken: narrantur\nHuman-readable POS: VERB\nspaCy POS key: 100\nM = nlp.get_pipe(\"morphologizer\")\npos_lookup = M.vocab.strings[100]\nprint(f'POS key: {tokens[1].pos}')\nprint(f'Human-readable POS: {pos_lookup}')\n\nPOS key: 100\nHuman-readable POS: VERB\nWe can use the label_data attribute from the morphologizer component to derive the complete (coarse-grain) tagset…\ndef split_pos(morph):\n    if 'POS=' in morph:\n        return morph.split('POS=')[1].split('|')[0]\n    else:\n        return None\n    \ntagset = sorted(list(set([split_pos(k) for k, v in M.label_data['morph'].items() if split_pos(k)])))\nprint(tagset)\n\n['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'VERB', 'X']\nSpaCy has an explain methods that can show human-readable descriptions of these standard tags…\ndata = []\n\nfor tag in tagset:\n    data.append([tag, spacy.explain(tag)])\n\nprint(tabulate.tabulate(data, headers=['TAG', 'Description']))\n\nTAG    Description\n-----  -------------------------\nADJ    adjective\nADP    adposition\nADV    adverb\nAUX    auxiliary\nCCONJ  coordinating conjunction\nDET    determiner\nINTJ   interjection\nNOUN   noun\nNUM    numeral\nPART   particle\nPRON   pronoun\nPROPN  proper noun\nPUNCT  punctuation\nSCONJ  subordinating conjunction\nVERB   verb\nX      other\nIt may also be useful to know the “confidence” of the tagger in making its decision. We can derive this from the output tagger’s model.predict method. This returns (at least, in part) a ranked list of per-scores, the maximum value of which determines the final annotation.\n# Helper function to get tagging scores\n\ndef get_tagging_scores(doc, n=3):\n    # cf. https://stackoverflow.com/a/69228515\n    scores = []\n    tagger = nlp.get_pipe('tagger')\n    labels = tagger.labels\n    for token in doc:\n        token_scores = tagger.model.predict([doc])[0][token.i]\n        r = [*enumerate(token_scores)]\n        r.sort(key=lambda x: x[1], reverse=True)\n        scores.append([(labels[i], p) for i, p in r[:n]])\n    return scores\n# Get the top 3 tags by score for each token in the Doc\n\ntagging_probs = get_tagging_scores(doc)\n\nfor token in doc:\n    print(f'Token: {token.text}', end='\\n\\n')\n    data = []\n    for label, prob in tagging_probs[token.i]:\n        data.append([label, prob])\n    print(tabulate.tabulate(data, headers=['Label', 'Score']))\n    break\n\nToken: Haec\n\nLabel         Score\n---------  --------\npronoun    10.7023\nadjective   8.26614\nnoun        3.13534",
    "crumbs": [
      "NLP Tasks",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>POS Tagging</span>"
    ]
  },
  {
    "objectID": "pos-tagging.html#pos-tagging-with-latincy",
    "href": "pos-tagging.html#pos-tagging-with-latincy",
    "title": "6  POS Tagging",
    "section": "",
    "text": "References\nNLTK Chapter 5 “Categorizing and tagging words” link\nSLP Chapter 8 “Sequence labeling for parts of speech and named entities” link\nspaCy Tagger and Morphologizer\n\n\n\n\nGamba, Federica, and Daniel Zeman. 2023. “Universalising Latin Universal Dependencies: A Harmonisation of Latin Treebanks in UD.” In Proceedings of the Sixth Workshop on Universal Dependencies (UDW, GURT/SyntaxFest 2023), edited by Loïc Grobol and Francis Tyers, 7–16. Washington, D.C.: Association for Computational Linguistics. https://aclanthology.org/2023.udw-1.2.",
    "crumbs": [
      "NLP Tasks",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>POS Tagging</span>"
    ]
  },
  {
    "objectID": "morph-tagging.html",
    "href": "morph-tagging.html",
    "title": "7  Morphological Tagging",
    "section": "",
    "text": "7.1 POS Tagging with LatinCy\nMorphological tagging is the task of mapping a token in a text to various morphological tags as appropriate for the token’s part of speech. A noun will have morphological tags for its gender, number, and case, while a verb—a finite verb, at least—will have tags for its person, number, tense, mood, and voice. The default LatinCy pipelines have a morphologizer component that assigns these tags.\n# Imports & setup\n\nimport spacy\nfrom pprint import pprint\nnlp = spacy.load('la_core_web_lg')\ntext = \"Haec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum. Avus eius Acrisius appellabatur.\"\ndoc = nlp(text)\nprint(doc)\n\nHaec narrantur a poetis de Perseo. Perseus filius erat Iovis, maximi deorum. Avus eius Acrisius appellabatur.\nMorphological tags are annotations of the Token objects and are stored in the morph attribute.\nsample_token = doc[1]\n\nprint(f'Sample token: {sample_token.text}')\nprint(f'Sample morph: {sample_token.morph}')\n\nSample token: narrantur\nSample morph: Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin|Voice=Pass\nNote that the morphological annotations are stored in a spaCy’s MorphAnalysis object…\ntype(sample_token.morph)\n\nspacy.tokens.morphanalysis.MorphAnalysis\nLatinCy users may find the following MorphAnalysis methods of use.\nTo get all of the morphological tags for a token as a Python dict, you can use the to_dict method…\nsample_morph_dict = sample_token.morph.to_dict()\npprint(sample_morph_dict)\n\n{'Mood': 'Ind',\n 'Number': 'Plur',\n 'Person': '3',\n 'Tense': 'Pres',\n 'VerbForm': 'Fin',\n 'Voice': 'Pass'}\nYou can also get the string representation of the morphological analysis with the to_json method:\nsample_morph_dict = sample_token.morph.to_json()\npprint(sample_morph_dict)\n\n'Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin|Voice=Pass'\nConversely, a MorphAnalysis object can be created from a Python dict using the set_morph method on a spaCy Token object.\ntext = \"Festina lente.\"\ndoc_no_annotations = nlp.make_doc(text)\nprint(f'{doc_no_annotations[0].text} -&gt; {doc_no_annotations[0].morph if doc_no_annotations[0].morph else \"{No morph data}\"}')\n\nFestina -&gt; {No morph data}\nfestina_dict = {\"Person\": \"2\", \"Number\": \"Singular\", \"Tense\": \"Present\", \"Mood\": \"Imperative\", \"Voice\": \"Active\"}\ndoc_no_annotations[0].set_morph(festina_dict)\nprint(f'{doc_no_annotations[0].text} -&gt; {doc_no_annotations[0].morph if doc_no_annotations[0].morph else \"{No morph data}\"}')\nprint(type(doc_no_annotations[0].morph))\n\nFestina -&gt; Mood=Imperative|Number=Singular|Person=2|Tense=Present|Voice=Active\n&lt;class 'spacy.tokens.morphanalysis.MorphAnalysis'&gt;\nAs noted above, the mophological tags are POS-specific. LatinCy uses a limited set of morphological keys based on the UD treebanks. Moreover, with respect to the tag values, one specific adjustment that has been made from the UD treebanks is that for verbs LatinCy uses the six “traditional” tense values, i.e. present, imperfect, future, perfect, pluperfect, and future perfect.\nimport tabulate\n\ndata = []\n\ndoc = nlp(\"Tum arcam ipsam in mare coniecit.\")\ntokens = [item for item in doc]\n\nfor token in tokens:\n    data.append([token.text, token.morph.to_json()])    \n\nprint(tabulate.tabulate(data, headers=['Text', \"Morph\"])) \n\nText      Morph\n--------  ---------------------------------------------------------------\nTum\narcam     Case=Acc|Gender=Fem|Number=Sing\nipsam     Case=Acc|Gender=Fem|Number=Sing\nin\nmare      Case=Acc|Gender=Neut|Number=Sing\nconiecit  Mood=Ind|Number=Sing|Person=3|Tense=Perf|VerbForm=Fin|Voice=Act\n.\nNote how in the sentence above the verb coniecit is tagged with the ‘Tense=Perf’ (for perfect), whereas in the UD treebanks this would have been annotated with ‘Tense=Past’ in coordination with ‘Aspect=Perf’.",
    "crumbs": [
      "NLP Tasks",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Morphological Tagging</span>"
    ]
  },
  {
    "objectID": "morph-tagging.html#pos-tagging-with-latincy",
    "href": "morph-tagging.html#pos-tagging-with-latincy",
    "title": "7  Morphological Tagging",
    "section": "",
    "text": "References\nNLTK Chapter 5 “Categorizing and tagging words” link\nspaCy Morphologizer",
    "crumbs": [
      "NLP Tasks",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Morphological Tagging</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Getting Started with LatinCy",
    "section": "",
    "text": "Preface\n“Getting Started with LatinCy” is an always-a-work-in-progress combination of documentation and demo notebooks for working with the LatinCy models on a variety of Latin text analysis and NLP tasks.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#key-links",
    "href": "index.html#key-links",
    "title": "Getting Started with LatinCy",
    "section": "Key links",
    "text": "Key links\n📦Models: https://huggingface.co/latincy\n🌌Universe: https://spacy.io/universe/project/latincy\n📝Preprint: https://arxiv.org/abs/2305.04365\nThis book has been written using Jupyter notebooks which have then been collated with Quarto. To learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Getting Started with LatinCy",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThank you to all of the readers who have submitted corrections via GitHub PRs: @sjhuskey.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "abbreviations.html",
    "href": "abbreviations.html",
    "title": "Abbreviations",
    "section": "",
    "text": "Where possible, I will include references to standard NLP works using the following abbreviations:\nNLTK Bird, S., Klein, E., and Loper, E. 2015. Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit. 2nd Edition. https://www.nltk.org/book/. (Bird, Klein, and Loper 2015)\nSLP Jurafsky, D., and Martin, J.H. 2020. Speech and Language Processing. 3rd Edition, Draft. https://web.stanford.edu/~jurafsky/slp3/. (Jurafsky and Martin 2020)\n\n\n\n\nBird, Steven, Ewan Klein, and Edward Loper. 2015. Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit. 2nd Edition. https://www.nltk.org/book/.\n\n\nJurafsky, Daniel, and James H. Martin. 2020. “Speech and Language Processing (3rd Edition, Draft).” https://web.stanford.edu/~jurafsky/slp3/.",
    "crumbs": [
      "Abbreviations"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Open LatinCy Projects",
    "section": "",
    "text": "Model-based enclitic splitting\nAs discussed in the Word Tokenization chapter, enclitic splitting is currently limited to que (and variants) and is a rules-based process that is part of the LatinCy custom tokenizer. It would be preferable to make enclitic splitting a separate pipeline component and moreover one that is model-based. This component could be placed immediately after the tokenizer and use the Retokenizer.split method to reconstruct token sequences where valid enclitics are identified. This would have the added advantage of allowing enclitic splitting to be “turned off”, so to speak, by removing the component from the pipeline.",
    "crumbs": [
      "Appendices",
      "Open LatinCy Projects"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bird, Steven, Ewan Klein, and Edward Loper. 2015. Natural\nLanguage Processing with Python:\nAnalyzing Text with the Natural\nLanguage Toolkit. 2nd Edition. https://www.nltk.org/book/.\n\n\nBurns, Patrick J. 2018. “Backoff Lemmatization as a\nPhilological Method.” In Digital\nHumanities 2018, DH 2018, Book of\nAbstracts, El Colegio de\nMéxico, UNAM, and RedHD,\nMexico City, Mexico,\nJune 26-29, 2018.\n\n\n———. 2020. “Ensemble Lemmatization with the Classical\nLanguage Toolkit.” Studi e Saggi\nLinguistici 58 (1): 157–76. https://doi.org/10.4454/ssl.v58i1.273.\n\n\nClayman, Dee. 1981. “Sentence Length in\nGreek Hexameter Poetry.”\nQuantitative Linguistics 11: 107–36. https://papers.ssrn.com/abstract=1627358.\n\n\nGamba, Federica, and Daniel Zeman. 2023. “Universalising\nLatin Universal Dependencies:\nA Harmonisation of Latin\nTreebanks in UD.” In Proceedings of\nthe Sixth Workshop on Universal\nDependencies (UDW,\nGURT/SyntaxFest 2023), edited by Loïc\nGrobol and Francis Tyers, 7–16. Washington, D.C.: Association for\nComputational Linguistics. https://aclanthology.org/2023.udw-1.2.\n\n\nJanson, Tore. 1964. “The Problems of\nMeasuring Sentence-Length in\nClassical Texts.” Studia\nLinguistica 18 (1): 26–36. https://doi.org/10.1111/j.1467-9582.1964.tb00443.x.\n\n\nJurafsky, Daniel, and James H. Martin. 2020. “Speech and\nLanguage Processing (3rd Edition,\nDraft).” https://web.stanford.edu/~jurafsky/slp3/.\n\n\nWake, William C. 1957. “Sentence-Length Distributions of\nGreek Authors.” Journal of the Royal Statistical\nSociety. Series A (General) 120 (3): 331–46. https://www.jstor.org/stable/2343104.",
    "crumbs": [
      "Appendices",
      "References"
    ]
  }
]